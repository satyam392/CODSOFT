import nltk
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

text = "Natural Language Processing is a fascinating field of Artificial Intelligence."

tokens = word_tokenize(text)
pos_tags = nltk.pos_tag(tokens)
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

stemmer = PorterStemmer()
stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]

lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]

print("Original Text:", text)
print("Tokens:", tokens)
print("POS Tags:", pos_tags)
print("After Stop Word Removal:", filtered_tokens)
print("After Stemming:", stemmed_tokens)
print("After Lemmatization:", lemmatized_tokens)
